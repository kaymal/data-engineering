{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Apache Airflow (or simply _Airflow_) is a platform to programmatically \n",
    "- Create (Author)\n",
    "- Schedule\n",
    "- Monitor \n",
    "\n",
    "workflows.\n",
    "\n",
    "We can use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes the tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n",
    "\n",
    "When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# airflow needs a home, ~/airflow is the default,\n",
    "# but you can lay foundation somewhere else if you prefer\n",
    "# (optional)\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# install from pypi using pip\n",
    "pip install apache-airflow\n",
    "\n",
    "# initialize the database\n",
    "airflow db init\n",
    "\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --role Admin \\\n",
    "    --email spiderman@superhero.org\n",
    "\n",
    "# start the web server, default port is 8080\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# start the scheduler\n",
    "# open a new terminal or else run webserver with ``-D`` option to run it as a daemon\n",
    "airflow scheduler\n",
    "\n",
    "# visit localhost:8080 in the browser and use the admin account you just\n",
    "# created to login. Enable the example_bash_operator dag in the home page\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow [-h] GROUP_OR_COMMAND ...\n",
      "\n",
      "positional arguments:\n",
      "  GROUP_OR_COMMAND\n",
      "\n",
      "    Groups:\n",
      "      celery         Celery components\n",
      "      config         View configuration\n",
      "      connections    Manage connections\n",
      "      dags           Manage DAGs\n",
      "      db             Database operations\n",
      "      kubernetes     Tools to help run the KubernetesExecutor\n",
      "      pools          Manage pools\n",
      "      providers      Display providers\n",
      "      roles          Manage roles\n",
      "      tasks          Manage tasks\n",
      "      users          Manage users\n",
      "      variables      Manage variables\n",
      "\n",
      "    Commands:\n",
      "      cheat-sheet    Display cheat sheet\n",
      "      info           Show information about current Airflow and environment\n",
      "      kerberos       Start a kerberos ticket renewer\n",
      "      plugins        Dump information about loaded plugins\n",
      "      rotate-fernet-key\n",
      "                     Rotate encrypted connection credentials and variables\n",
      "      scheduler      Start a scheduler instance\n",
      "      sync-perm      Update permissions for existing roles and DAGs\n",
      "      version        Show the version\n",
      "      webserver      Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Command line tool, subcommands\n",
    "!airflow -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a simle task\n",
    "```bash\n",
    "Airflow run <dag_id> <task_id> <start_date>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "```bash\n",
    "# Run a task instance\n",
    "airflow tasks run example_bash_operator runme_0 2021-01-01\n",
    "# Run a backfill over 2 days\n",
    "airflow dags backfill example_bash_operator \\\n",
    "    --start-date 2021-01-01 \\\n",
    "    --end-date 2021-01-02\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG\n",
    "\n",
    "> In Airflow, a pipeline is represented as a Directed Acyclic Graph or DAG. The nodes of the graph represent tasks that are executed. The directed connections between nodes represent dependencies between the tasks. Representing a data pipeline as a DAG makes much sense, as some tasks need to finish before others can start. [source](https://datacamp.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "# Default arguments (optional) that will be applied to the components of DAG\n",
    "default_args = {\n",
    "    'owner': 'name',\n",
    "    'email': 'name@gmail.com',  # for alerting\n",
    "    'start_date': datetime(2020, 1, 20),\n",
    "    'retries': 2\n",
    "}\n",
    "\n",
    "etl_dag = DAG('example_etl', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow shell command can provide a lot of useful information when creating and troubleshooting workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow dags [-h] COMMAND ...\n",
      "\n",
      "Manage DAGs\n",
      "\n",
      "positional arguments:\n",
      "  COMMAND\n",
      "    backfill      Run subsections of a DAG for a specified date range\n",
      "    delete        Delete all DB records related to the specified DAG\n",
      "    list          List all the DAGs\n",
      "    list-jobs     List the jobs\n",
      "    list-runs     List DAG runs given a DAG id\n",
      "    next-execution\n",
      "                  Get the next execution datetimes of a DAG\n",
      "    pause         Pause a DAG\n",
      "    report        Show DagBag loading report\n",
      "    show          Displays DAG's tasks with their dependencies\n",
      "    state         Get the status of a dag run\n",
      "    test          Execute one single DagRun\n",
      "    trigger       Trigger a DAG run\n",
      "    unpause       Resume a paused DAG\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help      show this help message and exit\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!airflow dags -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow Webserver and UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow webserver [-h] [-A ACCESS_LOGFILE] [-L ACCESS_LOGFORMAT] [-D]\n",
      "                         [-d] [-E ERROR_LOGFILE] [-H HOSTNAME] [-l LOG_FILE]\n",
      "                         [--pid [PID]] [-p PORT] [--ssl-cert SSL_CERT]\n",
      "                         [--ssl-key SSL_KEY] [--stderr STDERR]\n",
      "                         [--stdout STDOUT] [-t WORKER_TIMEOUT]\n",
      "                         [-k {sync,eventlet,gevent,tornado}] [-w WORKERS]\n",
      "\n",
      "Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -A ACCESS_LOGFILE, --access-logfile ACCESS_LOGFILE\n",
      "                        The logfile to store the webserver access log. Use '-' to print to stderr\n",
      "  -L ACCESS_LOGFORMAT, --access-logformat ACCESS_LOGFORMAT\n",
      "                        The access log format for gunicorn logs\n",
      "  -D, --daemon          Daemonize instead of running in the foreground\n",
      "  -d, --debug           Use the server that ships with Flask in debug mode\n",
      "  -E ERROR_LOGFILE, --error-logfile ERROR_LOGFILE\n",
      "                        The logfile to store the webserver error log. Use '-' to print to stderr\n",
      "  -H HOSTNAME, --hostname HOSTNAME\n",
      "                        Set the hostname on which to run the web server\n",
      "  -l LOG_FILE, --log-file LOG_FILE\n",
      "                        Location of the log file\n",
      "  --pid [PID]           PID file location\n",
      "  -p PORT, --port PORT  The port on which to run the server\n",
      "  --ssl-cert SSL_CERT   Path to the SSL certificate for the webserver\n",
      "  --ssl-key SSL_KEY     Path to the key to use with the SSL certificate\n",
      "  --stderr STDERR       Redirect stderr to this file\n",
      "  --stdout STDOUT       Redirect stdout to this file\n",
      "  -t WORKER_TIMEOUT, --worker-timeout WORKER_TIMEOUT\n",
      "                        The timeout for waiting on webserver workers\n",
      "  -k {sync,eventlet,gevent,tornado}, --workerclass {sync,eventlet,gevent,tornado}\n",
      "                        The worker class to use for Gunicorn\n",
      "  -w WORKERS, --workers WORKERS\n",
      "                        Number of workers to run the webserver on\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!airflow webserver -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start webserver on port 8051\n",
    "!airflow webserver -p 8051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAGs: Operators, Tasks, Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "Airflow Operators represent a single task (e.g. running a command, python script, etc.) in a workflow. There are various operators that perform different tasks. A list of operators can be found [here](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html). Some of those are:\n",
    "- `DummyOperator`\n",
    "- `BashOperator`\n",
    "- `PythonOperator`\n",
    "- `PostgresOperator`\n",
    "\n",
    "Note that operators generally don't share information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DummyOperator`\n",
    "DummyOperator can be used for troubleshooting or testing.\n",
    "\n",
    "```bash\n",
    "DummyOperator(task_id='example', dag=dag)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `BashOperator`\n",
    "\n",
    "- Allows to specify any given Shell command or script and add it to an Airflow workflow.\n",
    "- It is possible to specify environment variables for the bash command.\n",
    "- In Airflow, tilde character does not by default represent home directory. This can be fixed with env variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Execute a Command\n",
    "task1 = BashOperator(    \n",
    "    task_id='bash_example',  # Name that shows up in the UI\n",
    "    bash_command='echo \"Hello!\"',\n",
    "    dag=my_dag  # Add the task to the DAG\n",
    ")\n",
    "\n",
    "# Execute a Script\n",
    "task2 = BashOperator(\n",
    "    task_id='bash_script',\n",
    "    bash_command='runupdate.sh',\n",
    "    dag=my_dag)\n",
    "    \n",
    "task3 = BashOperator(\n",
    "    task_id='clean_address',\n",
    "    bash_command='cat address.txt | awk \"NF==10\" > cleaned.txt',\n",
    "    dag=my_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define multiple `BashOperator`s within a workflow (same DAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first operator\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh',\n",
    "    dag=analytics_dag)\n",
    "\n",
    "# Define the second operator\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh',\n",
    "    dag=analytics_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Tasks are instances of operators. Although tasks can be assigned to variables, a task is referred by its `task_id` instead of `variable name`.\n",
    "\n",
    "Task Dependencies and Task Ordering:\n",
    "\n",
    "Task dependencies are referred to as _upstream_ or _downstream_ tasks. Upstream tasks: are compleated prior to any downstream tasks. Since Airflow 1.8, dependencies are defined using the _bitshift_ operators.\n",
    "- Upstream operator: `>>`  (before)\n",
    "- Downstream operator: `<<`  (after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     bash_command='echo 1',\n",
    "                     dag=example_dag)\n",
    "\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                     bash_command='echo 2',\n",
    "                     dag=example_dag)\n",
    "\n",
    "# Set first_task to run before second_task\n",
    "task1 >> task2  # or task2 << task1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
