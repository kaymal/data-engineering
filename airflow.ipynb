{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Apache Airflow (or simply _Airflow_) is a platform to programmatically \n",
    "- Create (Author)\n",
    "- Schedule\n",
    "- Monitor \n",
    "\n",
    "workflows.\n",
    "\n",
    "We can use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes the tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n",
    "\n",
    "When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# airflow needs a home, ~/airflow is the default,\n",
    "# but you can lay foundation somewhere else if you prefer\n",
    "# (optional)\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# install from pypi using pip\n",
    "pip install apache-airflow\n",
    "\n",
    "# initialize the database\n",
    "airflow db init\n",
    "\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --role Admin \\\n",
    "    --email spiderman@superhero.org\n",
    "\n",
    "# start the web server, default port is 8080\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# start the scheduler\n",
    "# open a new terminal or else run webserver with ``-D`` option to run it as a daemon\n",
    "airflow scheduler\n",
    "\n",
    "# visit localhost:8080 in the browser and use the admin account you just\n",
    "# created to login. Enable the example_bash_operator dag in the home page\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow [-h] GROUP_OR_COMMAND ...\n",
      "\n",
      "positional arguments:\n",
      "  GROUP_OR_COMMAND\n",
      "\n",
      "    Groups:\n",
      "      celery         Celery components\n",
      "      config         View configuration\n",
      "      connections    Manage connections\n",
      "      dags           Manage DAGs\n",
      "      db             Database operations\n",
      "      kubernetes     Tools to help run the KubernetesExecutor\n",
      "      pools          Manage pools\n",
      "      providers      Display providers\n",
      "      roles          Manage roles\n",
      "      tasks          Manage tasks\n",
      "      users          Manage users\n",
      "      variables      Manage variables\n",
      "\n",
      "    Commands:\n",
      "      cheat-sheet    Display cheat sheet\n",
      "      info           Show information about current Airflow and environment\n",
      "      kerberos       Start a kerberos ticket renewer\n",
      "      plugins        Dump information about loaded plugins\n",
      "      rotate-fernet-key\n",
      "                     Rotate encrypted connection credentials and variables\n",
      "      scheduler      Start a scheduler instance\n",
      "      sync-perm      Update permissions for existing roles and DAGs\n",
      "      version        Show the version\n",
      "      webserver      Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Command line tool, subcommands\n",
    "!airflow -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a simle task\n",
    "```bash\n",
    "Airflow run <dag_id> <task_id> <start_date>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "```bash\n",
    "# Run a task instance\n",
    "airflow tasks run example_bash_operator runme_0 2021-01-01\n",
    "# Run a backfill over 2 days\n",
    "airflow dags backfill example_bash_operator \\\n",
    "    --start-date 2021-01-01 \\\n",
    "    --end-date 2021-01-02\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG\n",
    "\n",
    "> In Airflow, a pipeline is represented as a Directed Acyclic Graph or DAG. The nodes of the graph represent tasks that are executed. The directed connections between nodes represent dependencies between the tasks. Representing a data pipeline as a DAG makes much sense, as some tasks need to finish before others can start. [source](https://datacamp.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "# Default arguments (optional) that will be applied to the components of DAG\n",
    "default_args = {\n",
    "    'owner': 'name',\n",
    "    'email': 'name@gmail.com',  # for alerting\n",
    "    'start_date': datetime(2020, 1, 20),\n",
    "    'retries': 2\n",
    "}\n",
    "\n",
    "etl_dag = DAG('example_etl', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow shell command can provide a lot of useful information when creating and troubleshooting workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow dags [-h] COMMAND ...\n",
      "\n",
      "Manage DAGs\n",
      "\n",
      "positional arguments:\n",
      "  COMMAND\n",
      "    backfill      Run subsections of a DAG for a specified date range\n",
      "    delete        Delete all DB records related to the specified DAG\n",
      "    list          List all the DAGs\n",
      "    list-jobs     List the jobs\n",
      "    list-runs     List DAG runs given a DAG id\n",
      "    next-execution\n",
      "                  Get the next execution datetimes of a DAG\n",
      "    pause         Pause a DAG\n",
      "    report        Show DagBag loading report\n",
      "    show          Displays DAG's tasks with their dependencies\n",
      "    state         Get the status of a dag run\n",
      "    test          Execute one single DagRun\n",
      "    trigger       Trigger a DAG run\n",
      "    unpause       Resume a paused DAG\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help      show this help message and exit\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!airflow dags -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow Webserver and UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow webserver [-h] [-A ACCESS_LOGFILE] [-L ACCESS_LOGFORMAT] [-D]\n",
      "                         [-d] [-E ERROR_LOGFILE] [-H HOSTNAME] [-l LOG_FILE]\n",
      "                         [--pid [PID]] [-p PORT] [--ssl-cert SSL_CERT]\n",
      "                         [--ssl-key SSL_KEY] [--stderr STDERR]\n",
      "                         [--stdout STDOUT] [-t WORKER_TIMEOUT]\n",
      "                         [-k {sync,eventlet,gevent,tornado}] [-w WORKERS]\n",
      "\n",
      "Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -A ACCESS_LOGFILE, --access-logfile ACCESS_LOGFILE\n",
      "                        The logfile to store the webserver access log. Use '-' to print to stderr\n",
      "  -L ACCESS_LOGFORMAT, --access-logformat ACCESS_LOGFORMAT\n",
      "                        The access log format for gunicorn logs\n",
      "  -D, --daemon          Daemonize instead of running in the foreground\n",
      "  -d, --debug           Use the server that ships with Flask in debug mode\n",
      "  -E ERROR_LOGFILE, --error-logfile ERROR_LOGFILE\n",
      "                        The logfile to store the webserver error log. Use '-' to print to stderr\n",
      "  -H HOSTNAME, --hostname HOSTNAME\n",
      "                        Set the hostname on which to run the web server\n",
      "  -l LOG_FILE, --log-file LOG_FILE\n",
      "                        Location of the log file\n",
      "  --pid [PID]           PID file location\n",
      "  -p PORT, --port PORT  The port on which to run the server\n",
      "  --ssl-cert SSL_CERT   Path to the SSL certificate for the webserver\n",
      "  --ssl-key SSL_KEY     Path to the key to use with the SSL certificate\n",
      "  --stderr STDERR       Redirect stderr to this file\n",
      "  --stdout STDOUT       Redirect stdout to this file\n",
      "  -t WORKER_TIMEOUT, --worker-timeout WORKER_TIMEOUT\n",
      "                        The timeout for waiting on webserver workers\n",
      "  -k {sync,eventlet,gevent,tornado}, --workerclass {sync,eventlet,gevent,tornado}\n",
      "                        The worker class to use for Gunicorn\n",
      "  -w WORKERS, --workers WORKERS\n",
      "                        Number of workers to run the webserver on\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!airflow webserver -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start webserver on port 8051\n",
    "!airflow webserver -p 8051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAGs: Operators, Tasks, Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "Airflow Operators represent a single task (e.g. running a command, python script, etc.) in a workflow. There are various operators that perform different tasks. A list of operators can be found [here](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html). Some of those are:\n",
    "- `DummyOperator`\n",
    "- `BashOperator`\n",
    "- `PythonOperator`\n",
    "- `PostgresOperator`\n",
    "\n",
    "Note that operators generally don't share information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DummyOperator`\n",
    "DummyOperator can be used for troubleshooting or testing.\n",
    "\n",
    "```bash\n",
    "DummyOperator(task_id='example', dag=dag)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `BashOperator`\n",
    "\n",
    "- Allows to specify any given Shell command or script and add it to an Airflow workflow.\n",
    "- It is possible to specify environment variables for the bash command.\n",
    "- In Airflow, tilde character does not by default represent home directory. This can be fixed with env variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Execute a Command\n",
    "task1 = BashOperator(    \n",
    "    task_id='bash_example',  # Name that shows up in the UI\n",
    "    bash_command='echo \"Hello!\"',\n",
    "    dag=my_dag  # Add the task to the DAG\n",
    ")\n",
    "\n",
    "# Execute a Script\n",
    "task2 = BashOperator(\n",
    "    task_id='bash_script',\n",
    "    bash_command='runupdate.sh',\n",
    "    dag=my_dag)\n",
    "    \n",
    "task3 = BashOperator(\n",
    "    task_id='clean_address',\n",
    "    bash_command='cat address.txt | awk \"NF==10\" > cleaned.txt',\n",
    "    dag=my_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define multiple `BashOperator`s within a workflow (same DAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first operator\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh',\n",
    "    dag=analytics_dag)\n",
    "\n",
    "# Define the second operator\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh',\n",
    "    dag=analytics_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `PythonOperator`\n",
    "\n",
    "Operates similarly to the `Bash Operator`. It executes Python functions or callables, and can pass in arguments  to them. \n",
    "- Supports both positional and keyword arguments to tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def my_func():\n",
    "    print(\"Hallo!\")\n",
    "\n",
    "python_task = PythonOperator(\n",
    "    task_id='simple_print',\n",
    "    python_callable=my_func,\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep(sec):  \n",
    "    time.sleep(sec)\n",
    "\n",
    "sleep_task = PythonOperator(\n",
    "    task_id='sleep',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'sec': 1}  # Keywords must match\n",
    "    dag=example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Tasks are instances of operators. Although tasks can be assigned to variables, a task is referred by its `task_id` instead of `variable name`.\n",
    "\n",
    "Task Dependencies and Task Ordering:\n",
    "\n",
    "Task dependencies are referred to as _upstream_ or _downstream_ tasks. Upstream tasks are compleated prior to any downstream tasks. Since Airflow 1.8, dependencies are defined using the _bitshift_ operators.\n",
    "- Upstream operator: `>>`  (before)\n",
    "- Downstream operator: `<<`  (after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     bash_command='echo 1',\n",
    "                     dag=example_dag)\n",
    "\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                     bash_command='echo 2',\n",
    "                     dag=example_dag)\n",
    "\n",
    "# Set first_task to run before second_task\n",
    "task1 >> task2  # or task2 << task1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multistep Workflow with the `PythonOperator` and the `EmailOperator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "def get_file(url, path):\n",
    "    r = requests.get(url)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"File downloaded from {URL} and saved to {path}\")\n",
    "\n",
    "def parse_file(input_file, output_file):\n",
    "    # Parse file\n",
    "    # ..\n",
    "    # Save file\n",
    "    # ..\n",
    "\n",
    "# Pull file\n",
    "get_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_file_dag\n",
    ")\n",
    "\n",
    "# Parse file\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable = parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs = {'inputfile':'latest.json', 'outputfile':'parsed.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_file_dag\n",
    ")\n",
    "\n",
    "# Send file via email\n",
    "email_task = EmailOperator(\n",
    "    task_id='email_data_eng',\n",
    "    to='tk@gmail.com',\n",
    "    subject='Latest update',\n",
    "    html_content='Attached is the latest JSON file.',\n",
    "    files='parsed.json',\n",
    "    dag=process_file_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling\n",
    "\n",
    "A DAG Run is a specific instance of a workflow at a given  point in time.\n",
    "\n",
    "Some important arguments used when scheduling a DAG are:\n",
    "- `start_date`\n",
    "- `end_date`: optional\n",
    "- `schedule_interval`: optional. Using `cron` syntax or built-in presets.\n",
    "- `max_tries`\n",
    "\n",
    "Note that schedules can be modified via UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cron Presets**\n",
    "\n",
    "<div class=\"wy-table-responsive\"><table class=\"docutils align-default\">\n",
    "<colgroup>\n",
    "<col style=\"width: 16%\">\n",
    "<col style=\"width: 66%\">\n",
    "<col style=\"width: 18%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>preset</p></th>\n",
    "<th class=\"head\"><p>meaning</p></th>\n",
    "<th class=\"head\"><p>cron</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code></p></td>\n",
    "<td><p>Don't schedule, use for exclusively \"externally triggered\"\n",
    "DAGs</p></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">@once</span></code></p></td>\n",
    "<td><p>Schedule once and only once</p></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">@hourly</span></code></p></td>\n",
    "<td><p>Run once an hour at the beginning of the hour</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">*</span> <span class=\"pre\">*</span> <span class=\"pre\">*</span> <span class=\"pre\">*</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">@daily</span></code></p></td>\n",
    "<td><p>Run once a day at midnight</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">0</span> <span class=\"pre\">*</span> <span class=\"pre\">*</span> <span class=\"pre\">*</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">@weekly</span></code></p></td>\n",
    "<td><p>Run once a week at midnight on Sunday morning</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">0</span> <span class=\"pre\">*</span> <span class=\"pre\">*</span> <span class=\"pre\">0</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">@monthly</span></code></p></td>\n",
    "<td><p>Run once a month at midnight of the first day of the month</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">0</span> <span class=\"pre\">1</span> <span class=\"pre\">*</span> <span class=\"pre\">*</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">@quarterly</span></code></p></td>\n",
    "<td><p>Run once a quarter at midnight on the first day</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">0</span> <span class=\"pre\">1</span> <span class=\"pre\">*/3</span> <span class=\"pre\">*</span></code></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">@yearly</span></code></p></td>\n",
    "<td><p>Run once a year at midnight of January 1</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">0</span> <span class=\"pre\">1</span> <span class=\"pre\">1</span> <span class=\"pre\">*</span></code></p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table></div>\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner':'data_eng',\n",
    "    'start_date': datetime(2020, 2, 15),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "process_file_dag = DAG(\n",
    "    dag_id='process_file', \n",
    "    default_args=default_args, \n",
    "    schedule_interval='@monthly')\n",
    "\n",
    "\n",
    "def get_file(url, path):\n",
    "    r = requests.get(url)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"File downloaded from {URL} and saved to {path}\")\n",
    "\n",
    "\n",
    "def parse_file(inputfile, outputfile):\n",
    "    with open(inputfile) as infile:\n",
    "        data=json.load(infile)\n",
    "        with open(outputfile, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        \n",
    "# Pull file\n",
    "get_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://example/file.json', 'savepath':'latest.json'},\n",
    "    dag=process_file_dag\n",
    ")\n",
    "\n",
    "# Parse file\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable = parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs = {'inputfile':'latest.json', 'outputfile':'parsed.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_file_dag\n",
    ")\n",
    "\n",
    "# Send file via email\n",
    "email_task = EmailOperator(\n",
    "    task_id='email_data_eng',\n",
    "    to='tk@gmail.com',\n",
    "    subject='Latest update',\n",
    "    html_content='Attached is the latest JSON file.',\n",
    "    files='parsed.json',\n",
    "    dag=process_file_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensors\n",
    "\n",
    "A Sensor is an operator that waits for a condition such as a response from a web request, creation of a file or updating a database, to be true. Sensors are assigned to tasks. \n",
    "\n",
    "Sensors include, but not limited to:\n",
    "- `FileSensor`\n",
    "- `HttpSensor`\n",
    "- `ExternalTaskSensor`\n",
    "- `SqlSensor`\n",
    "\n",
    "Some arguments used when defining sensors are:\n",
    "- `mode`=`poke` to run repeatedly or `reschedule` to try later\n",
    "- `poke_interval`: time interval between checks\n",
    "- `timeout`\n",
    "\n",
    "**When to use a Sensor?**\n",
    "\n",
    "Understanding how sensors are used within Airflow provides a lot of flexibility when defining workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "file_sensor_task = FileSensor(task_id='file_sensor',\n",
    "                              filepath='data.csv',\n",
    "                              poke_interval=120,\n",
    "                              dag=report_dag)\n",
    "\n",
    "init_cleanup >> file_sensor_task >> generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executors\n",
    "\n",
    "An Executor runs the tasks defined within the workflow. Different executors handle running the tasks differently. Some executors are:\n",
    "- `SequentialExecutor`: Default executor which runs one task at a time. It is mainly suitable for development and debugging.\n",
    "- `LocalExcecutor`: Runs on a single system and treats task as processes allowing concurrent runs. It is a good choice for a single production Airflow system.\n",
    "- `CeleryExecutor`: Uses Celery backend to use multiple worker systems. Although powerful, it is difficult to set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executor = SequentialExecutor\n"
     ]
    }
   ],
   "source": [
    "!airflow config list | grep \"executor = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airflow.cfg         \u001b[34mlogs\u001b[m\u001b[m                webserver_config.py\n",
      "airflow.db          unittests.cfg\n"
     ]
    }
   ],
   "source": [
    "!ls ~/airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executor = SequentialExecutor\n"
     ]
    }
   ],
   "source": [
    "!cat ~/airflow/airflow.cfg | grep \"executor = \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executor is `SequentialExcecutor`, `poke` mode can cause problems if the sensor is not initiated. Changing mode to `reschedule` can give Airflow a chance to run another task while waiting for the `data.csv` file. Another option to solve this problem is modify the executor type (e.g. `LocalExecutor`) that will allow parallelism greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "my_dag = DAG(\n",
    "    dag_id = 'execute_this',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "check = FileSensor(\n",
    "    task_id='check_file',\n",
    "    filepath='data.csv',\n",
    "    start_date=datetime(2021,1,1),\n",
    "    mode='poke',  # Change this to 'reschedule'\n",
    "    dag=my_dag\n",
    ")\n",
    "\n",
    "do_this = BashOperator(\n",
    "    task_id='do_this',\n",
    "    bash_command='do_this.sh',\n",
    "    start_date=datetime(2021,1,1),\n",
    "    dag=my_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
