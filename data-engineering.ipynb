{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many differences in what Data Scientists and Data Engineers doing. Yet, a Data Scientist should be familiar to some level what Data Engineering really is. Below is the comparison of Data Engineer vs. Data Scientist tasks.\n",
    "\n",
    "![](http://community.datacamp.com.s3.amazonaws.com/community/production/ckeditor_assets/pictures/421/content_screenshot_2017-02-23_15_21_00.png)\n",
    "[source](https://www.datacamp.com/community/blog/data-scientist-vs-data-engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I used a dataset from [Kaggle](https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results) throughout the notebook._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "- The data engineer should make sure there's a separate database for analytics, so that the data scientists do not slow down the functioning of the application while directly querying the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Services\n",
    "\n",
    "There are many cloud providers. The ones with the highest share in the market are:\n",
    " - AWS\n",
    " - Azure\n",
    " - Google Cloud\n",
    " \n",
    "The main cloud services are:\n",
    " - Storage: Store unstructured data\n",
    "     - Amazon S3, Azure Blob Storage, Google Cloud Storage\n",
    " - Compute: Use VMs to do computation\n",
    "     - Amazon EC2, Azure Virtual Machines, Google Compute Engine\n",
    " - Database: Use databases for the structured data\n",
    "     - Amazon RDS, Azure SQL Database, Google Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases\n",
    "\n",
    "A database is a structured set of data held in a computer, especially one that is accessible in various ways. The two main types of modern databases are relational and non-relational, also known as SQL or NoSQL(for their query languages). SQL databases are known as relational databases, and have a table-based data structure, with a strict, predefined schema required. NoSQL databases, or non-relational databases, can be document based, graph databases, key-value pairs, or wide-column stores. NoSQL databases don’t require any predefined schema, allowing you to work more freely with “unstructured data.” Relational databases are vertically scalable, but usually more expensive, whereas the horizontal scaling nature of NoSQL databases is more cost efficient. Unlike traditional, SQL based, relational databases, NoSQL databases can store and process data in real-time. [ref](https://www.mongodb.com/scale/nosql-vs-relational-databases)\n",
    "\n",
    "A non-relational database is a database that does not use the tabular schema of rows and columns found in most traditional database systems. Instead, non-relational databases use a storage model that is optimized for the specific requirements of the type of data being stored. For example, data may be stored as simple key/value pairs, as JSON documents, or as a graph consisting of edges and vertices. [ref](https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/non-relational-data)\n",
    "\n",
    "![](https://i0.wp.com/www.jamesserra.com/wp-content/uploads/2015/08/Untitled-picture.png?ssl=1)\n",
    "[source](https://www.jamesserra.com/archive/2015/08/relational-databases-vs-non-relational-databases/)\n",
    "\n",
    "![](https://camo.githubusercontent.com/1d4a0fdf0b47403e7c95a5649703495381c279c3/68747470733a2f2f7777772e6e6574736f6c7574696f6e732e636f6d2f696e7369676874732f77702d636f6e74656e742f75706c6f6164732f323031342f30372f355f7468696e67735f796f755f6d7573745f636f6e73696465725f6265666f72655f6e6f73716c312e6a7067)\n",
    "[source](https://github.com/UWCoffeeNCode/resources/wiki/SQL-and-NoSQL-Databases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the first 3 rows of the DataFrame\n",
    "print(data.head(3))\n",
    "\n",
    "# Show the info of the DataFrame\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Computing\n",
    "\n",
    "Parallel computing can optimize the use of multiple processing units, and  optimize the use of memory between several machines. However, we cannot split every task successfully into subtasks. Additionally, some tasks might be too small to benefit from parallel computing due to the _communication overhead_.\n",
    "\n",
    "Benefits of parallel computing are\n",
    "- Processing power\n",
    "- Memory: partition the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use parallel computing using the the low-level python API  [multiprocessing.Pool](https://docs.python.org/3.8/library/multiprocessing.html), which allows us to distribute workload (or calculations) over several processes. _Note: `@print_timing` decorator is used to time each operation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "with Pool(5) as p:\n",
    "    print(p.map(f, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def take_mean_age(year_and_group):\n",
    "    year, group = year_and_group\n",
    "    return pd.DataFrame({\"Age\": group[\"Age\"].mean()}, index=[year])\n",
    "\n",
    "with Pool(4) as p:\n",
    "    results = p.map(take_mean_age, athlete_events.groupby(\"Year\"))\n",
    "\n",
    "result_df = pd.concat(results)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Computation Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more convenient way to parallelize an apply over several groups is using the [Dask](https://docs.dask.org/en/latest/) framework and its abstraction of the `pandas` DataFrame. \n",
    "\n",
    "Dask is a flexible library for parallel computing in Python. It is composed of two parts:\n",
    "\n",
    "1. Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\n",
    "2. “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'athlete_events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ef6205ca7dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Partition dataframe into 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mathlete_events_dask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mathlete_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Run parallel computations on each partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'athlete_events' is not defined"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd \n",
    "\n",
    "# Partition dataframe into 4\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "# Run parallel computations on each partition\n",
    "result_df = athlete_events_dask.groupby('Year').Age.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask vs Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the _DataFrame abstraction_ makes parallel computing easier and it is often used when dealing with Big Data, for example in Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally Dask is smaller and lighter weight than Spark. This means that it has fewer features and, instead, is used in conjunction with other libraries, particularly those in the numeric Python ecosystem. It couples with libraries like Pandas or Scikit-Learn to achieve high-level functionality. [source](https://docs.dask.org/en/latest/spark.html)\n",
    "\n",
    "> - Spark is mature and all-inclusive. If you want a single project that does everything and you’re already on Big Data hardware, then Spark is a safe bet, especially if your use cases are typical ETL + SQL and you’re already using Scala.\n",
    "- Dask is lighter weight and is easier to integrate into existing code and hardware. If your problems vary beyond typical ETL + SQL and you want to add flexible parallelism to existing solutions, then Dask may be a good fit, especially if you are already using Python and associated libraries like NumPy and Pandas. [source](https://docs.dask.org/en/latest/spark.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Hadoop, Hive, Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark\n",
    "\n",
    "Apache Spark is written in Scala programming language. PySpark has been released in order to support the collaboration of Apache Spark and Python, it actually is a Python API for Spark. In addition, PySpark, helps you interface with Resilient Distributed Datasets (RDDs) in Apache Spark and Python programming language. [source](https://databricks.com/glossary/pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "athlete_events_spark = (spark\n",
    "    .read\n",
    "    .csv(\"data/athlete_events.csv\",\n",
    "         header=True,\n",
    "         inferSchema=True,\n",
    "         escape='\"'))\n",
    "\n",
    "athlete_events_spark = (athlete_events_spark\n",
    "    .withColumn(\"Height\",\n",
    "                athlete_events_spark.Height.cast(\"integer\")))\n",
    "\n",
    "athlete_events_spark = (athlete_events_spark\n",
    "    .withColumn(\"Age\",\n",
    "                athlete_events_spark.Age.cast(\"integer\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Height: integer (nullable = true)\n",
      " |-- Weight: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- NOC: string (nullable = true)\n",
      " |-- Games: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Medal: string (nullable = true)\n",
      "\n",
      "None\n",
      "DataFrame[Year: int, avg(Age): double]\n",
      "+----+------------------+\n",
      "|Year|          avg(Age)|\n",
      "+----+------------------+\n",
      "|1896|23.580645161290324|\n",
      "|1924|28.373324544056253|\n",
      "|2006|25.959151072569604|\n",
      "|1908|26.970228384991845|\n",
      "|1952|26.161546085232903|\n",
      "|1956|25.926673567977915|\n",
      "|1988|24.079431552931485|\n",
      "|1994|24.422102596580114|\n",
      "|1968|24.248045555448314|\n",
      "|2014|25.987323655694134|\n",
      "|1904| 26.69814995131451|\n",
      "|2004|25.639514989213716|\n",
      "|1932| 32.58207957204948|\n",
      "|1996|24.915045018878885|\n",
      "|1998|25.163197335553704|\n",
      "|1960|25.168848457954294|\n",
      "|2012| 25.96137770897833|\n",
      "|1912| 27.53861997940268|\n",
      "|2016| 26.20791934541204|\n",
      "|1936|27.530328324986087|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age (Lazy Evaluation)\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running a PySpark File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyspark.sql import SparkSession\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "    athlete_events_spark = (spark\n",
      "        .read\n",
      "        .csv(\"data/athlete_events.csv\",\n",
      "             header=True,\n",
      "             inferSchema=True,\n",
      "             escape='\"'))\n",
      "\n",
      "    athlete_events_spark = (athlete_events_spark\n",
      "        .withColumn(\"Height\",\n",
      "                    athlete_events_spark.Height.cast(\"integer\")))\n",
      "\n",
      "    print(athlete_events_spark\n",
      "        .groupBy('Year')\n",
      "        .mean('Height')\n",
      "        .orderBy('Year')\n",
      "        .show())"
     ]
    }
   ],
   "source": [
    "!cat spark-script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll work with a local Spark instance running on 4 threads, to print a DataFrame with with average Olympian heights by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master local[4] spark-script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|       avg(Height)|\n",
      "+----+------------------+\n",
      "|1896| 172.7391304347826|\n",
      "|1900|176.63793103448276|\n",
      "|1904| 175.7887323943662|\n",
      "|1906|178.20622568093384|\n",
      "|1908|177.54315789473685|\n",
      "|1912| 177.4479889042996|\n",
      "|1920| 175.7522816166884|\n",
      "|1924|174.96303901437372|\n",
      "|1928| 175.1620512820513|\n",
      "|1932|174.22011541632315|\n",
      "|1936| 175.7239932885906|\n",
      "|1948|176.17279726261762|\n",
      "|1952|174.13893967093236|\n",
      "|1956|173.90096798212957|\n",
      "|1960|173.14128595600675|\n",
      "|1964|  173.448573701557|\n",
      "|1968| 173.9458648072826|\n",
      "|1972|174.56536284096757|\n",
      "|1976|174.92052773737794|\n",
      "|1980|175.52748832195473|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    athlete_events_spark = (spark\n",
    "        .read\n",
    "        .csv(\"data/athlete_events.csv\",\n",
    "             header=True,\n",
    "             inferSchema=True,\n",
    "             escape='\"'))\n",
    "\n",
    "    athlete_events_spark = (athlete_events_spark\n",
    "        .withColumn(\"Height\",\n",
    "                    athlete_events_spark.Height.cast(\"integer\")))\n",
    "\n",
    "    print(athlete_events_spark\n",
    "        .groupBy('Year')\n",
    "        .mean('Height')\n",
    "        .orderBy('Year')\n",
    "        .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Scheduling\n",
    "\n",
    "There are several tools that can help us scheduling Spark jobs. Some of those include [Apache Airflow](https://airflow.apache.org/docs/stable/), [Spotify's Luigi](https://github.com/spotify/luigi) and [cron](https://en.wikipedia.org/wiki/Cron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directed Acyclic Graph (DAG)\n",
    "\n",
    "A directed acyclic graph (DAG) is a finite directed graph with no directed cycles. That is, it consists of finitely many vertices and edges (also called arcs), with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently-directed sequence of edges that eventually loops back to v again. [wikipedia](https://en.wikipedia.org/wiki/Directed_acyclic_graph)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Topological_Ordering.svg/220px-Topological_Ordering.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Airflow\n",
    "\n",
    "Apache Airflow (or simply _Airflow_) is a platform to programmatically author, schedule, and monitor workflows.\n",
    "\n",
    "We can use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes the tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n",
    "\n",
    "When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n",
    "\n",
    "> In Airflow, a pipeline is represented as a Directed Acyclic Graph or DAG. The nodes of the graph represent tasks that are executed. The directed connections between nodes represent dependencies between the tasks. Representing a data pipeline as a DAG makes much sense, as some tasks need to finish before others can start. [source](https://datacamp.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assemble_frame \n",
    "\n",
    "---place_tires\n",
    "\n",
    "---assemble_body\n",
    "\n",
    "    ---apply_paint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.DiGraph() \n",
    "G.add_edges_from([(1, 2), (1, 3), (3, 4)]) \n",
    "  \n",
    "plt.figure(figsize =(9, 9)) \n",
    "nx.draw_networkx(G, with_label = True, node_color ='green') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_crontab notation :_\n",
    "\n",
    "![](https://www.buycpanel.com/wp-content/uploads/2018/04/cron-settings-e1525100308573.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other kinds of operators you can use in real life situations, for example `StartClusterOperator`, `SparkJobOperator`, or `PythonOperator`.\n",
    "\n",
    "- `BashOperator` - executes a bash command\n",
    "- `PythonOperator` - calls an arbitrary Python function\n",
    "- `EmailOperator` - sends an email\n",
    "- `SimpleHttpOperator` - sends an HTTP request\n",
    "- `MySqlOperator`, `SqliteOperator`, `PostgresOperator`, `MsSqlOperator`, `OracleOperator`, `JdbcOperator`, etc. - executes a SQL command\n",
    "- `Sensor` - waits for a certain time, file, database row, S3 key, etc…\n",
    "\n",
    "We can also come up with a custom operator as per your need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"example_dag\", ..., schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Define operations\n",
    "start_cluster = StartClusterOperator(task_id=\"start_cluster\", dag=dag) \n",
    "ingest_customer_data = SparkJobOperator(task_id=\"ingest_customer_data\", dag=dag) \n",
    "ingest_product_data = SparkJobOperator(task_id=\"ingest_product_data\", dag=dag) \n",
    "enrich_customer_data = PythonOperator(task_id=\"enrich_customer_data\", ..., dag = dag)\n",
    "\n",
    "# Set up dependency flow\n",
    "start_cluster.set_downstream(ingest_customer_data) \n",
    "ingest_customer_data.set_downstream(enrich_customer_data) \n",
    "ingest_product_data.set_downstream(enrich_customer_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Airflow\n",
    "\n",
    "Now we need to move `dag.py` file, containing the DAG  we defined, to the DAGs folder. Here are the steps to find it:\n",
    "\n",
    "- The airflow home directory is defined in the `AIRFLOW_HOME` environment variable. Type `echo $AIRFLOW_HOME` to find out.\n",
    "- In this directory, find the `airflow.cfg` file. Use `head` to read the file, and find the value of the `dags_folder`.\n",
    "\n",
    "We can find the folder and move the `dag.py` file there: `mv ~/dag.py <dags_folder>`.\n",
    "\n",
    "Now that we've placed the DAG file in the correct place, we can check out the Airflow Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Extract, Transform and Load (ETL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Extract\n",
    "\n",
    "We can extraact data from many sources such as databases, files (flat files, unstructured files..) or through APIs. \n",
    "\n",
    "#### API\n",
    "\n",
    "One way of extracting data is to use an API. In many cases the response is in JSON format, which is semi-structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'DoreenMichele', 'descendants': 0, 'id': 16222421, 'score': 1, 'time': 1516800277, 'title': 'First Blue Moon Total Eclipse in 150 Years Coming on Jan. 31', 'type': 'story', 'url': 'http://baltimore.cbslocal.com/2018/01/02/blue-moon-total-eclipse-jan-31/amp/'}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222421.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()['score']\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database\n",
    "\n",
    "We can extract data from a database using `SQLAlchemy` or other tools and libraries. This topic is further covered in my SQL repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract table to a pandas DataFrame\n",
    "def extract_table_to_pandas(tablename, db_engine):\n",
    "    query = \"SELECT * FROM {}\".format(tablename)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "# Connect to the database using the connection URI\n",
    "connection_uri = \"postgresql://user:password@localhost:5432/database\" \n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Extract the first table into a pandas DataFrame\n",
    "extract_table_to_pandas('table1', db_engine)\n",
    "\n",
    "# Extract the second table into a pandas DataFrame\n",
    "extract_table_to_pandas('table2', db_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations_for_user(user_id, threshold=4.5):\n",
    "    # Join with the courses table\n",
    "    query = \"\"\"\n",
    "    SELECT title, rating FROM recommendations\n",
    "        INNER JOIN courses ON courses.course_id = recommendations.course_id\n",
    "        WHERE user_id=%(user_id)s AND rating>%(threshold)s\n",
    "        ORDER BY rating DESC\n",
    "    \"\"\"\n",
    "    # Add the threshold parameter\n",
    "    predictions_df = pd.read_sql(query, db_engine, params = {\"user_id\": user_id, \n",
    "                                                           \"threshold\": threshold})\n",
    "    return predictions_df.title.values\n",
    "\n",
    "# Try the function you created\n",
    "print(recommendations_for_user(12, 4.65))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "`pandas` is a great library to transform data with small size. We may want to use PySpark for the big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data into PySpark\n",
    "\n",
    "import pyspark.sql\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/database\",\n",
    "                \"table1\",\n",
    "                properties = {\"user\":\"repl\",\"password\":\"password\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupBy and mean to aggregate the column\n",
    "ratings_per_film_df = rating_df.groupby('film_id').mean('rating')\n",
    "\n",
    "# Join the tables using the film_id column\n",
    "film_df_with_ratings = film_df.join(\n",
    "    ratings_per_film_df,\n",
    "    film_df.film_id==ratings_per_film_df.film_id\n",
    ")\n",
    "\n",
    "# Show the 5 first results\n",
    "print(film_df_with_ratings.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "After extracting and transforming data, we need to load it to ready for analytics. \n",
    "\n",
    "Two (other) categories of databases are: \n",
    "1. Analytics\n",
    "    - OLAP: Online analytical processing\n",
    "    - Typically column-oriented\n",
    "    - Parallelization\n",
    "    - Generally queries on subset of columns\n",
    "    - Aggregate queries\n",
    "2. Applications\n",
    "    - OLTP: Online transaction processing\n",
    "    - Typically row-oriented\n",
    "    - Lots of transactions\n",
    "    - Per record\n",
    "\n",
    "![](https://i0.wp.com/www.jamesserra.com/wp-content/uploads/2015/08/nosql.png?w=714&ssl=1)\n",
    "[source](https://www.jamesserra.com/archive/2015/08/relational-databases-vs-non-relational-databases/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the column-oriented databases is the Massively Parallel Processing (MPP) Database. They are optimmized for analytics and parallel processing. Therefore, files are often loaded into a MPP database in order to make it available for analysis. Usually `parquet` file format is used for this purpose (CSV might not be a good option, since files with columnar storage format fits best to MPP.) \n",
    "\n",
    "> [Apache Parquet](https://parquet.apache.org) is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "\n",
    "Some examples are Google BigQuery, Amazon Redshift and Azure SQL Data Warehouse. For instance, we can load file/data into Amazon Redshift in the following way:\n",
    "- Write files (eg. in parquet format) into Amazon S3\n",
    "\n",
    "```{python}\n",
    "# Pandas .to_parquet() method\n",
    "df.to_parquet(\"./s3://path/to/bucket/table.parquet\n",
    "\n",
    "# PySpark .write.parquet() method\n",
    "df.write.parquet(\"./s3://path/to/bucket/table.parquet\")\n",
    "```\n",
    "\n",
    "- Send a copy query to Redshift\n",
    "\n",
    "```{python}\n",
    "# Connect to Redshift and copy the data from S3 to Redshift\n",
    "COPY table\n",
    "FROM 's3://path/to/bucket/table.parquet' FORMAT as parquet\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also load the results of the _transformation_ into a PostgreSQL database using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/database\"\n",
    "db_engine_dwh = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Transformation on data\n",
    "recommendations = transform_find_recommendatins(ratings_df)\n",
    "\n",
    "# Load into PostgreSQL database\n",
    "recommendations.to_sql(\"recommendations\", \n",
    "                       db_engine,\n",
    "                       schema=\"store\", \n",
    "                       if_exists=\"replace\" # we could use append\n",
    "                      )\n",
    "\n",
    "# Run the query to fetch the data\n",
    "pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "def extract_table_to_df(tablename, db_engine):\n",
    "    return pd.read_sql(\"SELECT * FROM {}\".format(tablename), db_engine)\n",
    "\n",
    "# Transform\n",
    "def split_columns_transform(df, column, pat, suffixes): \n",
    "    # Some transformation...\n",
    "    \n",
    "# Load\n",
    "def load_df_into_dwh(film_df, tablename, schema, db_engine):\n",
    "    return pd.to_sql(tablename, # \"table1\"\n",
    "                     db_engine, # Note that we may want to load into a different database\n",
    "                     # schema=schema, \n",
    "                     if_exists=\"replace\" # we could use append\n",
    "                    )\n",
    "\n",
    "db_engines = { ... } # Needs to be configured \n",
    "\n",
    "# Define the ETL function\n",
    "def etl():\n",
    "    # Extract data\n",
    "    film_df = extract_table_to_df(\"film\", db_engines[\"store\"])\n",
    "    # Transform data\n",
    "    film_df = split_columns_transform(film_df, \"rental_rate\", \".\", [\"_dollar\", \"_cents\"]) \n",
    "    # Load data\n",
    "    load_df_into_dwh(film_df, \"film\", \"store\", db_engines[\"dwh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Define a DAG to run on a daily basis\n",
    "dag = DAG(dag_id=\"etl_pipeline\", \n",
    "          schedule_interval=\"0 0 * * *\")\n",
    "\n",
    "etl_task = PythonOperator(task_id=\"etl_task\", \n",
    "                          python_callable=etl, \n",
    "                          dag=dag) \n",
    "\n",
    "etl_task.set_upstream(wait_for_table)\n",
    "\n",
    "## We can enable the DAG by switching the left-hand slide from Off to On from the Airflow UI.\n",
    "\n",
    "\n",
    "# task_recommendations = PythonOperator(\n",
    "#     task_id=\"recommendations_task\",\n",
    "#     python_callable=etl,\n",
    "#     op_kwargs={\"db_engines\":db_engines},\n",
    "# )\n",
    "\n",
    "# etl_task.set_upstream(wait_for_this_task)\n",
    "\n",
    "# Saved as etl_dag.py in ~/airflow/dags/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Referances:\n",
    "- airflow.apache.org\n",
    "- wikipedia.org\n",
    "- datacamp.com\n",
    "- dask.org\n",
    "- databricks.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
