{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Pipeline\n",
    "\n",
    "A _Data Pipeline_ is a sum of tools and processes for performing data integration. It's a series of steps in which data is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Data ingestion is the transportation of data from assorted sources to a storage medium where it can be accessed, used, and analyzed by an organization. The destination is typically a data warehouse. \n",
    "\n",
    "Batch vs. streaming ingestion:\n",
    "\n",
    "- The most common kind of data ingestion is batch processing. Here, the ingestion layer periodically collects and groups source data and sends it to the destination system. Groups may be processed based on any logical ordering, the activation of certain conditions, or a simple schedule. When having near-real-time data is not important, batch processing is typically used, since it’s generally easier and more affordably implemented than streaming ingestion.\n",
    "\n",
    "- Real-time processing (also called stream processing or streaming) involves no grouping at all. Data is sourced, manipulated, and loaded as soon as it’s created or recognized by the data ingestion layer. This kind of ingestion is more expensive, since it requires systems to constantly monitor sources and accept new information. However, it may be appropriate for analytics that require continually refreshed data.\n",
    "\n",
    "[ref](https://www.stitchdata.com/resources/data-ingestion/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion with Singer\n",
    "\n",
    "[Singer](https://www.singer.io) is defined as the open-source standard for writing scripts that move data.\n",
    "\n",
    "> Singer describes how data extraction scripts—called “taps” —and data loading scripts—called “targets”— should communicate, allowing them to be used in any combination to move data from any source to any destination. Send data between databases, web APIs, files, queues, and just about anything else you can think of.\n",
    "\n",
    "Singer applications communicate with JSON, making them easy to work with and implement in any programming language. Singer also supports JSON Schema to provide rich data types and rigid structure when needed. Additionally, many configuration files in Singer hold JSON. Therefore it is good to know how to write some configuration details of a database to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import json\n",
    "import json\n",
    "\n",
    "database_address = {\n",
    "  \"host\": \"10.0.0.5\",\n",
    "  \"port\": 1234\n",
    "}\n",
    "\n",
    "# Open the configuration file in writable mode\n",
    "with open(\"data/dp/database_config.json\", \"w\") as fh:\n",
    "  # Serialize the object in this file handle\n",
    "  json.dump(obj=database_address, fp=fh) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: The difference between `json.dumps` and `json.dump` is that, the former transforms the object to a string, whereas the latter writes the string to a file._\n",
    "\n",
    "JSON Schema is a vocabulary that allows us to annotate and validate JSON documents. Below is and example JSON schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the JSON schema\n",
    "schema = {'properties': {\n",
    "    'brand': {'type': 'string'},\n",
    "    'model': {'type': 'string'},\n",
    "    'price': {'type': 'number'},\n",
    "    'currency': {'type': 'string'},\n",
    "    'quantity': {'type': 'integer', 'minimum': 1},  \n",
    "    'date': {'type': 'string', 'format': 'date'},\n",
    "    'countrycode': {'type': 'string', 'pattern': \"^[A-Z]{2}$\"}, \n",
    "    'store_name': {'type': 'string'}}}\n",
    "\n",
    "# Write the schema\n",
    "singer.write_schema(stream_name=\"products\", schema=schema, key_properties=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'items': [{'brand': 'Huggies',\n",
    "            'model': 'newborn',\n",
    "            'price': 6.8,\n",
    "            'currency': 'EUR',            \n",
    "            'quantity': 40,\n",
    "            'date': '2019-02-01',\n",
    "            'countrycode': 'DE'            \n",
    "            },\n",
    "           {…}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singer extracts and loads data with _taps_ and _targets_, which are extracting and loading scripts and can be written in any programming language. \n",
    "- Singer Taps: Taps extract data from any source and write it to a standard stream in a JSON-based format.\n",
    "- Singer Targets: Targets consume data from taps and do something with it, like load it into a file, API or database.\n",
    "\n",
    "These can easily be mixed and matched to create small data pipelines that move data.\n",
    "\n",
    "Taps and Targest communicate over streams:\n",
    "- schema (metadata)\n",
    "- state (process metadata)\n",
    "- record (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Transform Load (ETL) and Extract Load Transform (ELT)\n",
    "\n",
    ">\"ETL is normally a continuous, ongoing process with a well-defined workflow. ETL first extracts data from homogeneous or heterogeneous data sources. Then, data is cleansed, enriched, transformed, and stored either back in the lake or in a data warehouse.\n",
    "\n",
    ">\"ELT (Extract, Load, Transform) is a variant of ETL wherein the extracted data is first loaded into the target system. Transformations are performed after the data is loaded into the data warehouse. ELT typically works well when the target system is powerful enough to handle transformations. Analytical databases like Amazon Redshift and Google BigQ.\" \n",
    "\n",
    "[Source](https://www.xplenty.com/blog/etl-vs-elt/)\n",
    "\n",
    "https://www.quora.com/What-is-the-difference-between-the-ETL-and-ELT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation, Testing Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing and Orchestrating the Workflow with Apache Airflow\n",
    "\n",
    "Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
